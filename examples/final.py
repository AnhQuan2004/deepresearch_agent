import os
import sys
import re
import json
import glob
import asyncio
import random
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass
from typing import List, Dict, Any, Optional, Tuple

# === ƒë·∫£m b·∫£o import ƒë∆∞·ª£c deep_researcher ===
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from deep_researcher import DeepResearcher

# ============ C·∫§U H√åNH CHUNG ============
# Multi-query
MAX_CONCURRENCY = 4          # s·ªë query ch·∫°y song song t·ªëi ƒëa
PER_QUERY_TIMEOUT_MIN = 100  # timeout m·ªói query (ph√∫t)
OUTPUT_ROOT = Path("reports")

# Aggregation -> JSON
OUTPUT_JSON = Path("answers.json")
GEMINI_MODEL = "gemini-2.5-pro"

# Google Generative AI API Key (∆∞u ti√™n ENV r·ªìi t·ªõi DEFAULT)
GOOGLE_API_KEY_DEFAULT = ""  # ƒêi·ªÅn key c·ªßa b·∫°n ·ªü ƒë√¢y n·∫øu kh√¥ng d√πng env

# File ch·ª©a queries (m·ªói d√≤ng l√† 1 query)
QUERIES_FILE = Path(__file__).parent / "queries.txt"

# Validator & x·ª≠ l√Ω
STRICT_END_INDEX = True        # bullet PH·∫¢I k·∫øt th√∫c b·∫±ng " [n]" ho·∫∑c " [n, m]"
PRUNE_UNUSED_CITATIONS = True  # t·ª± c·∫Øt citations kh√¥ng d√πng
FAIL_ON_INVALID_IDS = False    # True -> fail c·ª©ng n·∫øu l·ªói format
WARN_WEAK_SOURCES = True       # c·∫£nh b√°o ngu·ªìn y·∫øu
MAX_REPORT_CHARS = 0           # 0 = kh√¥ng c·∫Øt; v√≠ d·ª• 120_000 n·∫øu report c·ª±c d√†i

# ============ System prompt cho agent ============
SYSTEM_PROMPT = """B·∫°n l√† m·ªôt chuy√™n gia ph√¢n t√≠ch th√¥ng tin. Nhi·ªám v·ª•: ƒë·ªçc m·ªôt [B√°o c√°o] v√† tr·∫£ l·ªùi ch√≠nh x√°c, c√¥ ƒë·ªçng cho [C√¢u h·ªèi] duy nh·∫•t d·ª±a DUY NH·∫§T v√†o n·ªôi dung [B√°o c√°o].

ƒê·∫¶U V√ÄO:
- [C√¢u h·ªèi]: chu·ªói vƒÉn b·∫£n
- [B√°o c√°o]: to√†n b·ªô text c·ªßa b√°o c√°o, c√≥ th·ªÉ k√®m ph·∫ßn References v·ªõi c√°c ch·ªâ s·ªë [n].

ƒê·∫¶U RA (B·∫ÆT BU·ªòC JSON, KH√îNG TH√äM VƒÇN B·∫¢N NGO√ÄI JSON):
{
  "question": "string",
  "verdict": "C√≥ | Kh√¥ng | Kh√¥ng c√≥ ƒë·ªß th√¥ng tin",
  "detail_bullets": [
    "m·ªói bullet l√† m·ªôt m·ªánh ƒë·ªÅ s·ª± th·∫≠t tr·ª±c ti·∫øp, KH√îNG d√πng c√°c c·ª•m nh∆∞ 'B√°o c√°o cho bi·∫øt‚Ä¶'; PH·∫¢I k·∫øt th√∫c b·∫±ng kho·∫£ng tr·∫Øng r·ªìi t·ªõi ch·ªâ m·ª•c d·∫°ng [n] ho·∫∑c [n, m, ...], v√≠ d·ª•: 'Ph√≠ trung b√¨nh d∆∞·ªõi 0,04 USD. [2, 5]'"
  ],
  "citations": [
    {"id": 1, "url": "https://..."},
    {"id": 2, "url": "https://..."}
  ]
}

QUY T·∫ÆC TR√çCH D·∫™N:
1) KH√îNG b·ªãa ngu·ªìn. Ch·ªâ s·ª≠ d·ª•ng URL/ngu·ªìn th·ª±c s·ª± xu·∫•t hi·ªán trong [B√°o c√°o] (k·ªÉ c·∫£ ph·∫ßn 'References' n·∫øu c√≥).
2) N·∫øu [B√°o c√°o] c√≥ s·∫µn ch·ªâ m·ª•c [n] ‚Üí URL trong ph·∫ßn References, H√ÉY T√ÅI S·ª¨ D·ª§NG ƒë√∫ng s·ªë ƒë√≥ trong 'citations' v√† trong 'detail_bullets'.
3) N·∫øu [B√°o c√°o] KH√îNG c√≥ ch·ªâ m·ª•c s·∫µn, h√£y t·ª± ƒê√ÅNH S·ªê M·ªöI b·∫Øt ƒë·∫ßu t·ª´ 1 theo th·ª© t·ª± xu·∫•t hi·ªán, v√† d√πng NH·∫§T QU√ÅN gi·ªØa 'detail_bullets' v√† 'citations'.
4) M·ªói bullet trong 'detail_bullets' PH·∫¢I k·∫øt th√∫c b·∫±ng ' [..]' ch·ª©a √≠t nh·∫•t 1 id h·ª£p l·ªá; v√≠ d·ª•: '... [3]' ho·∫∑c '... [1, 4]'.
5) 'citations' l√† danh s√°ch {id,url}; id l√† s·ªë nguy√™n d∆∞∆°ng. Ch·ªâ URL tr·ª±c ti·∫øp, kh√¥ng m√¥ t·∫£ th√™m.

L∆ØU √ù:
- N·∫øu kh√¥ng ƒë·ªß d·ªØ ki·ªán trong [B√°o c√°o], ƒë·∫∑t 'verdict' = 'Kh√¥ng c√≥ ƒë·ªß th√¥ng tin' v√† ch·ªâ t·∫°o c√°c bullet n√™u r√µ thi·∫øu d·ªØ ki·ªán (c≈©ng c√≥ [n] n·∫øu c√≥ ngu·ªìn n√≥i thi·∫øu).
- Xu·∫•t ra JSON-OBJECT h·ª£p l·ªá duy nh·∫•t, kh√¥ng c√≥ b·∫•t k·ª≥ b√¨nh lu·∫≠n hay text ngo√†i JSON.
"""

# ============ Utility Functions ============

def slugify(text: str, max_len: int = 60) -> str:
    """Chuy·ªÉn text th√†nh slug an to√†n cho t√™n th∆∞ m·ª•c"""
    text = re.sub(r"\s+", " ", text).strip().lower()
    text = re.sub(r"[^\w\s-]", "", text, flags=re.UNICODE)
    text = text.replace(" ", "-")
    return text[:max_len] if len(text) > max_len else text

def load_queries_from_file(file_path: Path) -> List[str]:
    """ƒê·ªçc queries t·ª´ file, m·ªói d√≤ng l√† 1 query"""
    queries = []
    if file_path.exists():
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    # B·ªè qua d√≤ng tr·ªëng v√† comment (b·∫Øt ƒë·∫ßu b·∫±ng #)
                    if line and not line.startswith("#"):
                        queries.append(line)
            print(f"ƒê√£ load {len(queries)} queries t·ª´ {file_path}")
        except Exception as e:
            print(f"L·ªói khi ƒë·ªçc file queries: {e}")
    else:
        print(f"File {file_path} kh√¥ng t·ªìn t·∫°i")
    return queries

# ============ Multi Query (DeepResearcher) ============

def make_manager():
    """T·∫°o instance m·ªõi c·ªßa DeepResearcher cho m·ªói query"""
    return DeepResearcher(
        max_iterations=3,
        max_time_minutes=PER_QUERY_TIMEOUT_MIN - 1,  # ƒë·ªÉ n·ªôi b·ªô d·ª´ng tr∆∞·ªõc timeout c·ª©ng
        verbose=True,
        tracing=True
    )

async def run_one_query(query: str, sem: asyncio.Semaphore) -> Dict[str, Any]:
    """Ch·∫°y m·ªôt query v·ªõi DeepResearcher"""
    slug = slugify(query)
    ts = datetime.now().strftime("%Y%m%d-%H%M%S")
    out_dir = OUTPUT_ROOT / f"{ts}-{slug}"
    out_dir.mkdir(parents=True, exist_ok=True)

    report_path = out_dir / "report.txt"
    meta_path = out_dir / "meta.txt"

    # Ghi meta ban ƒë·∫ßu
    meta = [
        f"query: {query}",
        f"slug: {slug}",
        f"time: {ts}",
        f"started: {datetime.now().isoformat()}"
    ]
    meta_path.write_text("\n".join(meta), encoding="utf-8")

    manager = make_manager()
    print(f"[START] {slug[:40]}...")
    
    async with sem:
        try:
            timeout = PER_QUERY_TIMEOUT_MIN * 60
            report = await asyncio.wait_for(manager.run(query), timeout=timeout)

            # L∆∞u report
            report_path.write_text(report, encoding="utf-8")
            
            # Update meta
            with open(meta_path, "a", encoding="utf-8") as f:
                f.write(f"\nlen_chars: {len(report)}")
                f.write(f"\nstatus: success")
                f.write(f"\ncompleted: {datetime.now().isoformat()}\n")

            print(f"[DONE ] {slug[:40]}... -> {report_path}")
            return {
                "query": query,
                "slug": slug,
                "dir": str(out_dir),
                "ok": True,
                "error": None
            }
            
        except asyncio.TimeoutError:
            err = f"Timeout sau {PER_QUERY_TIMEOUT_MIN} ph√∫t"
            print(f"[FAIL ] {slug[:40]}... -> {err}")
            
            with open(meta_path, "a", encoding="utf-8") as f:
                f.write(f"\nerror: {err}")
                f.write(f"\nstatus: timeout")
                f.write(f"\nfailed: {datetime.now().isoformat()}\n")
            
            # L∆∞u k·∫øt qu·∫£ partial n·∫øu c√≥
            try:
                if hasattr(manager, "_last_partial_result") and manager._last_partial_result:
                    partial_path = out_dir / "partial_report.txt"
                    partial_path.write_text(manager._last_partial_result, encoding="utf-8")
                    print(f"[INFO ] L∆∞u k·∫øt qu·∫£ m·ªôt ph·∫ßn v√†o {partial_path}")
            except Exception as e2:
                print(f"[WARN ] Kh√¥ng th·ªÉ l∆∞u partial: {e2}")
                
            return {
                "query": query,
                "slug": slug,
                "dir": str(out_dir),
                "ok": False,
                "error": err
            }
            
        except Exception as e:
            err = str(e)
            print(f"[FAIL ] {slug[:40]}... -> {err}")
            
            with open(meta_path, "a", encoding="utf-8") as f:
                f.write(f"\nerror: {err}")
                f.write(f"\nstatus: error")
                f.write(f"\nfailed: {datetime.now().isoformat()}\n")
            
            # L∆∞u k·∫øt qu·∫£ partial n·∫øu c√≥
            try:
                if hasattr(manager, "_last_partial_result") and manager._last_partial_result:
                    partial_path = out_dir / "partial_report.txt"
                    partial_path.write_text(manager._last_partial_result, encoding="utf-8")
                    print(f"[INFO ] L∆∞u k·∫øt qu·∫£ m·ªôt ph·∫ßn v√†o {partial_path}")
            except Exception as e2:
                print(f"[WARN ] Kh√¥ng th·ªÉ l∆∞u partial: {e2}")
                
            return {
                "query": query,
                "slug": slug,
                "dir": str(out_dir),
                "ok": False,
                "error": err
            }

async def run_many_queries(queries: List[str]) -> List[Dict[str, Any]]:
    """Ch·∫°y nhi·ªÅu queries song song"""
    OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)
    sem = asyncio.Semaphore(MAX_CONCURRENCY)
    
    # Ch·∫°y c√°c tasks
    tasks = [run_one_query(q, sem) for q in queries]
    results = await asyncio.gather(*tasks)

    # T·∫°o index cho batch n√†y
    index_path = OUTPUT_ROOT / "index.md"
    lines = [
        "# Reports Index",
        f"\nGenerated: {datetime.now().isoformat()}\n"
    ]
    
    for r in results:
        status = "‚úÖ" if r["ok"] else "‚ùå"
        rel = os.path.relpath(r["dir"], OUTPUT_ROOT)
        lines.append(f"- {status} `{r['slug']}` ‚Äî [{rel}](./{rel})")
        lines.append(f"  - Query: {r['query'][:100]}...")
        if r["error"]:
            lines.append(f"  - Error: {r['error']}")
    
    index_path.write_text("\n".join(lines) + "\n", encoding="utf-8")

    ok_count = sum(1 for r in results if r["ok"])
    print(f"\nT·ªïng k·∫øt DeepResearcher: {ok_count}/{len(results)} th√†nh c√¥ng.")
    print(f"M·ª•c l·ª•c: {index_path}\n")
    
    return results

# ============ Agent (Gemini 2.5 Pro) ============

import google.generativeai as genai

def configure_gemini():
    """C·∫•u h√¨nh Gemini API"""
    key = os.getenv("GOOGLE_API_KEY", "").strip()
    if not key:
        key = GOOGLE_API_KEY_DEFAULT.strip()
    if not key:
        print("‚ùå L·ªói: Thi·∫øu GOOGLE_API_KEY. Vui l√≤ng:")
        print("   - Set bi·∫øn m√¥i tr∆∞·ªùng GOOGLE_API_KEY, ho·∫∑c")
        print("   - ƒêi·ªÅn v√†o GOOGLE_API_KEY_DEFAULT trong file")
        sys.exit(1)
    genai.configure(api_key=key)
    print("‚úÖ ƒê√£ c·∫•u h√¨nh Gemini API")

def build_model():
    """T·∫°o Gemini model v·ªõi c·∫•u h√¨nh JSON response"""
    return genai.GenerativeModel(
        model_name=GEMINI_MODEL,
        system_instruction=SYSTEM_PROMPT,
        generation_config={
            "temperature": 0.2,
            "response_mime_type": "application/json",
        },
    )

def _read_txt(p: Path) -> Optional[str]:
    """ƒê·ªçc file text an to√†n"""
    try:
        if p.exists():
            return p.read_text(encoding="utf-8", errors="ignore")
    except Exception as e:
        print(f"[WARN] Kh√¥ng th·ªÉ ƒë·ªçc {p}: {e}")
    return None

def _truncate_report_if_needed(txt: str) -> str:
    """C·∫Øt report n·∫øu qu√° d√†i"""
    if MAX_REPORT_CHARS > 0 and len(txt) > MAX_REPORT_CHARS:
        return txt[:MAX_REPORT_CHARS] + "\n\n[TRUNCATED]\n"
    return txt

def detect_question(report_dir: Path, fallback: str = "C√¢u h·ªèi m·∫∑c ƒë·ªãnh") -> str:
    """Ph√°t hi·ªán c√¢u h·ªèi t·ª´ meta ho·∫∑c question.txt"""
    # ∆Øu ti√™n l·∫•y t·ª´ meta.txt
    meta = _read_txt(report_dir / "meta.txt")
    if meta:
        for line in meta.splitlines():
            if line.lower().startswith("query:"):
                query = line.split(":", 1)[1].strip()
                if query:
                    return query
    
    # Fallback sang question.txt
    qtxt = _read_txt(report_dir / "question.txt")
    if qtxt and qtxt.strip():
        return qtxt.strip()
    
    return fallback

def _extract_json_from_response(resp) -> str:
    """Extract JSON text t·ª´ Gemini response"""
    # C√°ch 1: L·∫•y t·ª´ text attribute
    text = getattr(resp, "text", None)
    if text and text.strip():
        return text.strip()
    
    # C√°ch 2: Parse t·ª´ response dict
    try:
        d = resp.to_dict()
        cands = d.get("candidates", [])
        chunks = []
        for c in cands:
            content = c.get("content") or {}
            parts = content.get("parts") or []
            for part in parts:
                t = part.get("text")
                if t:
                    chunks.append(t)
        text2 = "".join(chunks).strip()
        if text2:
            return text2
    except Exception:
        pass
    
    return ""

async def call_gemini_json(model, question: str, report_text: str, timeout_sec: int = 150) -> Dict[str, Any]:
    """G·ªçi Gemini v√† parse JSON response"""
    payload = {
        "C√¢u h·ªèi": question,
        "B√°o c√°o": report_text
    }
    prompt = json.dumps(payload, ensure_ascii=False)

    def _sync_call():
        return model.generate_content(
            contents=[{"role": "user", "parts": [prompt]}]
        )

    resp = await asyncio.wait_for(
        asyncio.to_thread(_sync_call),
        timeout=timeout_sec
    )
    
    text = _extract_json_from_response(resp)
    if not text:
        raise RuntimeError("Gemini returned empty text.")
    
    # Parse JSON
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        # Th·ª≠ extract JSON object t·ª´ text
        m = re.search(r"(\{.*\})", text, flags=re.DOTALL)
        if not m:
            raise
        return json.loads(m.group(1))

# ============ Validate & Normalize ============

ID_LIST_RE = re.compile(r"\[(\s*\d+(?:\s*,\s*\d+)*\s*)\]")

WEAK_SOURCE_PATTERNS = [
    r"scribd\.com",
    r"coursehero\.com",
    r"medium\.com",
    r"blogspot\.",
    r"wordpress\.",
    r"b2binpay\.com",
    r"coincodex\.com/blog",
    r"reddit\.com",
]

def is_weak_source(url: str) -> bool:
    """Ki·ªÉm tra URL c√≥ ph·∫£i ngu·ªìn y·∫øu kh√¥ng"""
    u = url.lower()
    return any(re.search(p, u) for p in WEAK_SOURCE_PATTERNS)

def normalize_payload(data: Dict[str, Any]) -> Dict[str, Any]:
    """Chu·∫©n h√≥a payload t·ª´ Gemini"""
    q = str(data.get("question", "")).strip()
    v = str(data.get("verdict", "")).strip()
    
    # Normalize bullets
    bullets = data.get("detail_bullets") or []
    if not isinstance(bullets, list):
        bullets = [str(bullets)]
    bullets = [str(x).strip() for x in bullets if str(x).strip()]

    # Normalize citations
    cites = data.get("citations") or []
    norm_cites: List[Dict[str, Any]] = []
    
    if isinstance(cites, list):
        for it in cites:
            if isinstance(it, dict) and "id" in it and "url" in it:
                try:
                    cid = int(it["id"])
                    curl = str(it["url"]).strip()
                    if cid > 0 and curl:
                        norm_cites.append({"id": cid, "url": curl})
                except Exception:
                    continue

    # Dedup by id
    id_to_url = {}
    for c in norm_cites:
        id_to_url[c["id"]] = c["url"]
    norm_cites = [{"id": k, "url": id_to_url[k]} for k in sorted(id_to_url.keys())]

    return {
        "question": q,
        "verdict": v,
        "detail_bullets": bullets,
        "citations": norm_cites
    }

def collect_used_ids(bullets: List[str]) -> Tuple[set, List[str]]:
    """Thu th·∫≠p c√°c ID ƒë∆∞·ª£c s·ª≠ d·ª•ng trong bullets"""
    used = set()
    errs = []
    
    for i, b in enumerate(bullets, 1):
        if STRICT_END_INDEX and not b.endswith("]"):
            errs.append(f"Bullet #{i} kh√¥ng k·∫øt th√∫c b·∫±ng ch·ªâ m·ª•c [..]")
        
        matches = list(ID_LIST_RE.finditer(b))
        if not matches:
            errs.append(f"Bullet #{i} kh√¥ng c√≥ ch·ªâ m·ª•c [n]")
            continue
        
        # L·∫•y match cu·ªëi c√πng (th∆∞·ªùng l√† citation)
        ids_str = matches[-1].group(1)
        for num in re.split(r"\s*,\s*", ids_str.strip()):
            if not re.fullmatch(r"\d+", num):
                errs.append(f"Bullet #{i} ch·ª©a ch·ªâ m·ª•c kh√¥ng h·ª£p l·ªá: '{num}'")
                continue
            used.add(int(num))
    
    return used, errs

def validate_and_prune(payload: Dict[str, Any]) -> Tuple[Dict[str, Any], List[str], List[str]]:
    """Validate v√† prune citations kh√¥ng d√πng"""
    bullets = payload.get("detail_bullets") or []
    cites = payload.get("citations") or []
    errors: List[str] = []
    warnings: List[str] = []

    # Ki·ªÉm tra [n/a]
    for i, b in enumerate(bullets, 1):
        if re.search(r"\[(?:\s*n/?a\s*)\]", b, flags=re.IGNORECASE):
            errors.append(f"Bullet #{i} d√πng [n/a] (kh√¥ng h·ª£p l·ªá)")

    # Thu th·∫≠p used IDs
    used_ids, errs_b = collect_used_ids(bullets)
    errors.extend(errs_b)

    # Ki·ªÉm tra tham chi·∫øu
    id_set = {c["id"] for c in cites if isinstance(c, dict) and "id" in c}
    for nid in sorted(used_ids):
        if nid not in id_set:
            errors.append(f"Tham chi·∫øu id [{nid}] kh√¥ng t·ªìn t·∫°i trong citations")

    # Prune citations kh√¥ng d√πng
    unused = sorted(list(id_set - used_ids))
    if unused:
        msg = f"Citations kh√¥ng ƒë∆∞·ª£c tham chi·∫øu: {unused}"
        if PRUNE_UNUSED_CITATIONS:
            cites = [c for c in cites if c["id"] in used_ids]
            payload["citations"] = cites
            msg += " ‚Üí ƒê√É C·∫ÆT B·ªé"
        warnings.append(msg)

    # C·∫£nh b√°o ngu·ªìn y·∫øu
    if WARN_WEAK_SOURCES:
        weak = [c for c in cites if is_weak_source(c["url"])]
        if weak:
            weak_msg = "Ngu·ªìn c√≥ ƒë·ªô tin c·∫≠y th·∫•p: "
            weak_details = [f'[{c["id"]}] {c["url"][:50]}...' for c in weak]
            warnings.append(weak_msg + ", ".join(weak_details))

    return payload, errors, warnings

# ============ Orchestrator ============

@dataclass
class AgentUnit:
    report_dir: str
    payload: Optional[Dict[str, Any]]
    error: Optional[str]
    warnings: List[str]

async def aggregate_dirs_to_json(dirs: List[str]) -> List[AgentUnit]:
    """Aggregate c√°c report th√†nh JSON v·ªõi Gemini"""
    configure_gemini()
    model = build_model()

    async def process_dir(d: str) -> AgentUnit:
        report_dir = Path(d)
        
        # ƒê·ªçc report (∆∞u ti√™n report.txt, fallback sang partial_report.txt)
        report_text = _read_txt(report_dir / "report.txt")
        if not report_text or not report_text.strip():
            report_text = _read_txt(report_dir / "partial_report.txt")
        
        if not report_text or not report_text.strip():
            return AgentUnit(d, None, "Report r·ªóng ho·∫∑c kh√¥ng t·ªìn t·∫°i", [])

        question = detect_question(report_dir)
        report_text = _truncate_report_if_needed(report_text)

        last_err = None
        for attempt in range(3):
            try:
                data = await call_gemini_json(model, question, report_text, timeout_sec=150)
                payload = normalize_payload(data)
                payload, errs, warns = validate_and_prune(payload)

                if errs and FAIL_ON_INVALID_IDS:
                    return AgentUnit(d, None, "; ".join(errs), warns)

                soft_error = "; ".join(errs) if errs else None
                return AgentUnit(d, payload, soft_error, warns)
                
            except asyncio.TimeoutError:
                last_err = f"Timeout khi g·ªçi Gemini (attempt {attempt+1}/3)"
            except Exception as e:
                last_err = f"{type(e).__name__}: {e}"

            # Backoff ng·∫Øn
            await asyncio.sleep((1.6 ** attempt) + random.uniform(0, 0.5))

        return AgentUnit(d, None, last_err or "Gemini error", [])

    # Process song song v·ªõi semaphore
    sem = asyncio.Semaphore(MAX_CONCURRENCY)
    
    async def wrapped(d):
        async with sem:
            print(f"[AGENT] Processing {Path(d).name}")
            return await process_dir(d)

    tasks = [asyncio.create_task(wrapped(d)) for d in dirs]
    results = await asyncio.gather(*tasks)
    return results

# ============ Main Function ============

async def main():
    """Main orchestrator function"""
    
    # 1) Load queries
    queries = load_queries_from_file(QUERIES_FILE)
    
    # N·∫øu kh√¥ng c√≥ queries t·ª´ file, d√πng default
    if not queries:
        print("‚ö†Ô∏è Kh√¥ng c√≥ queries trong file, s·ª≠ d·ª•ng queries m·∫∑c ƒë·ªãnh")
        queries = [
            "T·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng k√©p h√†ng nƒÉm (CAGR) c·ªßa lƒ©nh v·ª±c d·ª± √°n bnb ho·∫°t ƒë·ªông c√≥ ƒë∆∞·ª£c d·ª± b√°o > 20% kh√¥ng?"
        ]
    
    print(f"\nüìã S·ªë l∆∞·ª£ng queries: {len(queries)}")
    for i, q in enumerate(queries, 1):
        print(f"   {i}. {q[:80]}...")

    # 2) Ch·∫°y DeepResearcher song song
    print("\n" + "="*60)
    print("üöÄ PHASE 1: RUN MULTI-QUERY (DeepResearcher)")
    print("="*60)
    
    results = await run_many_queries(queries)

    # L·∫•y danh s√°ch th∆∞ m·ª•c c√≥ report
    dirs_for_agent: List[str] = []
    for r in results:
        d = Path(r["dir"])
        if (d / "report.txt").exists() or (d / "partial_report.txt").exists():
            dirs_for_agent.append(r["dir"])

    if not dirs_for_agent:
        print("‚ùå Kh√¥ng c√≥ report n√†o ƒë·ªÉ t·ªïng h·ª£p.")
        return

    # 3) G·ªçi agent (Gemini) ƒë·ªÉ t·ªïng h·ª£p
    print("\n" + "="*60)
    print("ü§ñ PHASE 2: RUN AGENT (Gemini)")
    print("="*60)
    
    agent_units = await aggregate_dirs_to_json(dirs_for_agent)

    # 4) T·ªïng h·ª£p k·∫øt qu·∫£
    out: List[Dict[str, Any]] = []
    ok_count = 0
    warn_count = 0
    err_count = 0

    for u in agent_units:
        if u.payload:
            item = {
                "report_dir": u.report_dir,
                **u.payload,
                "error": u.error
            }
            if u.warnings:
                item["warnings"] = u.warnings
                warn_count += len(u.warnings)
            out.append(item)
            
            if not u.error:
                ok_count += 1
            else:
                err_count += 1
        else:
            # Fallback cho case l·ªói
            item = {
                "report_dir": u.report_dir,
                "question": detect_question(Path(u.report_dir)),
                "verdict": "Kh√¥ng c√≥ ƒë·ªß th√¥ng tin",
                "detail_bullets": [],
                "citations": [],
                "error": u.error or "Unknown error"
            }
            out.append(item)
            err_count += 1

    # 5) Ghi JSON output
    OUTPUT_JSON.write_text(
        json.dumps(out, ensure_ascii=False, indent=2),
        encoding="utf-8"
    )

    # 6) In summary
    print("\n" + "="*60)
    print("üìä SUMMARY")
    print("="*60)
    print(f"üìÅ Batch reports   : {len(dirs_for_agent)}")
    print(f"‚úÖ Valid (no error): {ok_count}")
    print(f"‚ö†Ô∏è  Soft errors     : {err_count}")
    print(f"‚ö° Warnings        : {warn_count}")
    print(f"üíæ JSON output     : {OUTPUT_JSON}")
    print("="*60)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n‚õî B·ªã h·ªßy b·ªüi ng∆∞·ªùi d√πng.")
        sys.exit(130)
    except Exception as e:
        print(f"\n‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
